%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{subcaption}
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{hyperref} % Used for linking to websites
\usepackage{amsmath, amsthm, amssymb} % Required for writing equations
\usepackage{pythonhighlight} % Required for including Python code
\usepackage{siunitx} % Required for scientific notation

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkFirstAuthorName, \hmwkSecondAuthorName} % Top left header
\chead{\hmwkClass\ (\hmwkClassTime): \hmwkTitle} % Top center head
%\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Perl} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Perl, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\perlscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.pl}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
%\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
%\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
%\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems
\setcounter{homeworkProblemCounter}{0}

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Part \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{Assignment\ \#3} % Assignment title
\newcommand{\hmwkDueDate}{Monday,\ March\ 19,\ 2018} % Due date
\newcommand{\hmwkClass}{CSC411} % Course/class
\newcommand{\hmwkClassTime}{L2501} % Class/lecture time
\newcommand{\hmwkFirstAuthorName}{Chawla Dhruv} % First name
\newcommand{\hmwkSecondAuthorName}{Maham Shama Saila} % First name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}
\vspace{3in}
}

\author{\textbf{\hmwkFirstAuthorName, \hmwkSecondAuthorName}}
%\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle
\clearpage

%----------------------------------------------------------------------------------------
%   PART 1
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\textit{Dataset Description}\\

The dataset is present in \texttt{clean\_fake.txt} and \texttt{clean\_real.txt}. Each line of the files contains a headline all in lower case. Example: \texttt{trump warns of vote flipping on machines} is present in \texttt{clean\_fake.txt}.\\

It does seem feasible to predict whether a headline is real or fake. There are certain phrases whose presence seems to indicate if a headline is real or not:

\begin{enumerate}
    \item \texttt{clean}: appears in 1 real headline and 5 fake headlines
    \item \texttt{hillary}: appears in 24 real headlines and 150 fake headlines
    \item \texttt{donald}: appears in 832 real headlines and 231 fake headlines
\end{enumerate}

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 2
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\textit{Naive Bayes Algorithm}\\

The Naive Bayes algorithm is applied on the dataset in \texttt{naive\_bayes.py}.\\

Our goal is to compute $P(fake | w)$ given $P(w | fake)$.\\

For a test headline, assume $w_i = 1$ if headline contains the word $w_i$ and $w_i = 0$ otherwise.\\

Then for training:\\
\begin{align*}
\hat{P}(w_i = 1 | fake) &= \frac {number\_of\_fake\_headlines\_containing\_w_i + m\hat{p}} {number\_of\_fake\_headlines + m}\\
\hat{P}(w_i = 0 | fake) &= 1 - \hat{P}(w_i = 1 | fake)\\
\hat{P}(w_i = 1 | real) &= \frac {number\_of\_real\_headlines\_containing\_w_i + m\hat{p}} {number\_of\_real\_headlines + m}\\
\hat{P}(w_i = 0 | real) &= 1 - \hat{P}(w_i = 1 | real)\\
\hat{P}(fake) &= \frac {number\_of\_fake\_headlines} {number\_of\_total\_headlines}\\
\hat{P}(real) &= 1 - \hat{P}(fake)\\
\end{align*}

For classifying:\\
\begin{align*}
\hat{P}(fake | w_1, w_2, ..., w_n) &\propto \hat{P}(fake) \prod_{i=1}^n \hat{P}(w_i | fake)\\
\hat{P}(real | w_1, w_2, ..., w_n) &\propto \hat{P}(real) \prod_{i=1}^n \hat{P}(w_i | real)\\
\hat{P}(fake | w_1, w_2, ..., w_n) &= \frac {\hat{P}(fake) \prod_{i=1}^n \hat{P}(w_i | fake)} {\hat{P}(fake) \prod_{i=1}^n \hat{P}(w_i | fake) + \hat{P}(real) \prod_{i=1}^n \hat{P}(w_i | real)}
\end{align*}

If $\hat{P}(fake | w_1, w_2, ..., w_n) >= 0.5$, the headline was classified as fake and otherwise, real.\\

Note: since $\prod_{i=1}^n \hat{P}(w_i | real)$ and $\prod_{i=1}^n \hat{P}(w_i | fake)$ invloves computing products of a lot of really small numbers (which might result in underflow), the property that $a_1, a_2, ..., a_n = exp(log a_1 + log a_2 + ... + log a_n)$ was used to compute the product.\\

The values of $m$ and $\hat{p}$ were determined using random search over the performance of validation set.\\

$m$ is the number of examples to be included in the prior calculation. The more number of examples, the more $\hat{p}$ influences the final probability calculation. Values of $m$ were tried on a logarithmic scale of $1, 10, 100, 1000$. Out of these, $m = 1$ gave the best result.\\

$\hat{p}$ is the prior probability of the word being real or fake. Values of $\hat{p}$ were tried in $0.1, 0.5, 0.7, 1$. Out of these $\hat{p} = 1$ gave the best performance.\\

Note: $\hat{p}$ was used as prior in calculation for both word being real and fake. This finding (low $m$ and $\hat{p}$) seems to imply that our prior assumptions in this case are not very accurate and it was best to have prior influence as minimal as possible.\\

The final results were as follows:
\begin{enumerate}
    \item Training Set: $97.28\%$
    \item Validation Set: $83.46\%$
    \item Testing Set: $85.68\%$
\end{enumerate}

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 3
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\textit{Naive Bayes Algorithm: Indicative Words}\\

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 4
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\textit{Logistic Regression}\\

PyTorch framework was used to create a Logistic Regression model. This model is constructed and trained in \texttt{logistic\_classifier.py}. The code is reproduced below.\\

\begin{python}
# Model
class LogisticRegression(nn.Module):
    def __init__(self, input_size, num_classes):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(input_size, num_classes)
    
    def forward(self, x):
        out = self.linear(x)
        return out

def train_LR_model(training_set, training_label, validation_set, validation_label, total_unique_words):
    """
    Trains Logistic Regression Numpy model

    PARAMETERS
    ----------
    training_set, validation_set: numpy arrays [num_examples, total_unique_words]
        For each headline in a set:
        v[k] = 1 if kth word appears in the headline else 0

    training_label, validation_label: numpy arrays [num_examples, [0, 1] or [1, 0]]
        [0, 1] if headline is fake else [1, 0]

    total_unique_words: int
        total number of unique words in training_set, validation_set, testing_set

    RETURNS
    -------
    model: LogisticRegression instance
        fully trained Logistic Regression model

    REQUIRES
    --------
    LogisticRegression: PyTorch class defined
    """
    # Hyper Parameters 
    input_size = total_unique_words
    num_classes = 2
    num_epochs = 800
    learning_rate = 0.001
    reg_lambda = 0.01

    model = LogisticRegression(input_size, num_classes)

    x = Variable(torch.from_numpy(training_set), requires_grad=False).type(torch.FloatTensor)
    y_classes = Variable(torch.from_numpy(np.argmax(training_label, 1)), requires_grad=False).type(torch.LongTensor)

    # Loss and Optimizer
    # Softmax is internally computed.
    # Set parameters to be updated.
    loss_fn = nn.CrossEntropyLoss()  
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  

    # Training the Model
    for epoch in range(num_epochs+1):
        # Forward + Backward + Optimize
        optimizer.zero_grad()
        outputs = model(x)
        l2_reg = Variable(torch.FloatTensor(1), requires_grad=True)
        for W in model.parameters():
            l2_reg = l2_reg + W.norm(2)
        loss = loss_fn(outputs, y_classes) + reg_lambda*l2_reg
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print("Epoch: " + str(epoch))

            # Training Performance
            x_train = Variable(torch.from_numpy(training_set), requires_grad=False).type(torch.FloatTensor)
            y_pred = model(x_train).data.numpy()
            train_perf_i = (np.mean(np.argmax(y_pred, 1) == np.argmax(training_label, 1))) * 100
            print("Training Set Performance  : " + str(train_perf_i) + "%")      

            # Validation Performance  
            x_valid = Variable(torch.from_numpy(validation_set), requires_grad=False).type(torch.FloatTensor)
            y_pred = model(x_valid).data.numpy()
            valid_perf_i = (np.mean(np.argmax(y_pred, 1) == np.argmax(validation_label, 1))) * 100
            print("Validation Set Performance:  " + str(valid_perf_i) + "%\n")

    return model
\end{python}

The final results were as follows:
\begin{enumerate}
    \item Training Set: $98.46\%$
    \item Validation Set: $82.04\%$
    \item Testing Set: $84.86\%$
\end{enumerate}

The learning curves are shown in Figure 1.\\

For regularization parameters, both $L1$ and $L2$ regularization were tried. $L2$ regularization performed $10\%$ better. The $\lambda$ values for $L2$ regularization was varied for values $0.001,0.001,0.05,0.01,0.1,0.5$. Out of these, $0.01$ performed the best.\\

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 5
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 6
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\textit{Logistic Regression: Indicative Words}\\

\textbf{Part 6(a) and 6(b)}\\

This part can be reproduced by calling \texttt{part6()}. The results are reproduced here:

\begin{verbatim}
Top 10 positive thetas (including stop-words):
1: trumps
2: tax
3: australia
4: tapping
5: says
6: debate
7: latest
8: hacking
9: business
10: asia


Top 10 negative thetas (including stop-words):
1: victory
2: breaking
3: information
4: elect
5: veterans
6: predicts
7: black
8: watch
9: d
10: won


Top 10 positive thetas (excluding stop-words):
1: trumps
2: tax
3: australia
4: tapping
5: says
6: debate
7: latest
8: hacking
9: business
10: asia


Top 10 negative thetas (excluding stop-words):
1: victory
2: breaking
3: information
4: elect
5: veterans
6: predicts
7: black
8: watch
9: d
10: won
\end{verbatim}

\textbf{Part 6(c)}\\

Using magnitude of the logistic regression parameters to indicate importance of a feature can be a bad idea in general if features are not normalized. 

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 7
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------
%   PART 8
%----------------------------------------------------------------------------------------

\begin{homeworkProblem}

\end{homeworkProblem}
\clearpage

%----------------------------------------------------------------------------------------

\end{document}